/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/**
 * Get card points for zapzap calculation
 * Jokers (52-53) = 0 points
 */
export declare function getCardPoints(cardId: number): number
/** Get card rank (0-12), returns 255 for jokers */
export declare function getRank(cardId: number): number
/** Get card suit (0-3), returns 255 for jokers */
export declare function getSuit(cardId: number): number
/** Check if card is a joker */
export declare function isJoker(cardId: number): boolean
/**
 * Calculate total hand value (for zapzap eligibility)
 * Jokers count as 0 points
 */
export declare function calculateHandValue(hand: Array<number>): number
/**
 * Calculate hand score for end of round
 * Jokers = 0 if lowest hand, 25 otherwise
 */
export declare function calculateHandScore(hand: Array<number>, isLowest: boolean): number
/** Check if player can call ZapZap (hand value <= 5) */
export declare function canCallZapzap(hand: Array<number>): boolean
/** Check if cards form a valid same-rank combination */
export declare function isValidSameRank(cards: Array<number>): boolean
/** Check if cards form a valid sequence */
export declare function isValidSequence(cards: Array<number>): boolean
/** Check if a play is valid */
export declare function isValidPlay(cards: Array<number>): boolean
/** Find all valid same-rank plays in hand */
export declare function findSameRankPlays(hand: Array<number>): Array<Array<number>>
/** Find all valid sequence plays in hand */
export declare function findSequencePlays(hand: Array<number>): Array<Array<number>>
/** Find all valid plays in hand */
export declare function findAllValidPlays(hand: Array<number>): Array<Array<number>>
/** Find the play that removes the most points from hand */
export declare function findMaxPointPlay(hand: Array<number>): Array<number> | null
/** Run benchmark: find all valid plays N times */
export declare function benchmarkFindAllValidPlays(hand: Array<number>, iterations: number): number
/** Game result from simulation */
export interface NativeGameResult {
  winner: number
  totalRounds: number
  finalScores: Array<number>
  wasGoldenScore: boolean
  playerCount: number
}
/**
 * Run a single game with specified strategies
 * Strategy types: "random", "hard", "hard_vince", "drl", "thibot"
 */
export declare function runGame(strategyTypes: Array<string>, seed?: number | undefined | null): NativeGameResult
/** Run multiple games and return statistics */
export interface BatchGameStats {
  gamesPlayed: number
  wins: Array<number>
  avgRounds: number
  totalTimeMs: number
  gamesPerSecond: number
}
/**
 * Run multiple games in batch for performance testing
 * Strategy types: "random", "hard", "hard_vince", "drl", "thibot"
 */
export declare function runGamesBatch(strategyTypes: Array<string>, gameCount: number, baseSeed?: number | undefined | null): BatchGameStats
/** Result of running games with transition collection */
export interface TrainingBatchResult {
  /** Total games played */
  gamesPlayed: number
  /** Wins per player */
  wins: Array<number>
  /** Total transitions collected */
  transitionsCollected: number
  /** Games per second */
  gamesPerSecond: number
}
/**
 * Run multiple games with transition collection for DRL training
 * Collects transitions and adds them directly to the trainer's replay buffer
 * Automatically syncs weights from trainer to DRL strategies for each game
 */
export declare function runTrainingBatch(strategyTypes: Array<string>, gameCount: number, drlPlayerIndex: number, epsilon: number, baseSeed?: number | undefined | null): TrainingBatchResult
/** Benchmark game simulation performance */
export declare function benchmarkSimulation(playerCount: number, gameCount: number): number
/** Get feature dimension (45) */
export declare function getFeatureDimension(): number
/**
 * Extract features from a game state for ML
 * Returns 45-dimensional feature vector
 */
export declare function extractFeatures(hand: Array<number>, playerIndex: number, scores: Array<number>, opponentHandSizes: Array<number>, roundNumber: number, deckSize: number, lastCardsPlayed: Array<number>, isGoldenScore: boolean, eliminatedPlayers: Array<number>): Array<number>
/** Extract features for hand size decision (before cards are dealt) */
export declare function extractHandSizeFeatures(activePlayerCount: number, isGoldenScore: boolean, myScore: number): Array<number>
/** Benchmark feature extraction */
export declare function benchmarkFeatureExtraction(iterations: number): number
/** Initialize the DQN with random weights */
export declare function dqnInit(seed?: number | undefined | null): boolean
/**
 * Get Q-values for a decision type
 * decision_type: "handSize", "zapzap", "playType", "drawSource"
 */
export declare function dqnPredict(features: Array<number>, decisionType: string): Array<number>
/** Select action using epsilon-greedy policy */
export declare function dqnSelectAction(features: Array<number>, decisionType: string, epsilon: number): number
/** Get greedy action (best Q-value) */
export declare function dqnGreedyAction(features: Array<number>, decisionType: string): number
/** Benchmark DQN inference */
export declare function benchmarkDqnInference(iterations: number): number
/** Training configuration exposed to JS */
export interface NativeTrainingConfig {
  /** Total number of games to train on */
  totalGames: number
  /** Number of games per batch */
  gamesPerBatch: number
  /** Batch size for training */
  batchSize: number
  /** Learning rate */
  learningRate: number
  /** Starting epsilon for exploration */
  epsilonStart: number
  /** Ending epsilon for exploration */
  epsilonEnd: number
  /** Number of games over which to decay epsilon */
  epsilonDecay: number
  /** Discount factor (gamma) */
  gamma: number
  /** Soft update rate (tau) */
  tau: number
  /** Replay buffer capacity */
  bufferCapacity: number
  /** Target network update frequency */
  targetUpdateFreq: number
}
/** Training state exposed to JS */
export interface NativeTrainingState {
  /** Number of games played */
  gamesPlayed: number
  /** Number of training steps */
  steps: number
  /** Current epsilon value */
  epsilon: number
  /** Average loss (recent) */
  avgLoss: number
  /** Average reward (recent) */
  avgReward: number
  /** Win rate (recent) */
  winRate: number
  /** Games per second */
  gamesPerSecond: number
  /** Whether training is currently running */
  isTraining: boolean
}
/** Create a new trainer with the given configuration */
export declare function trainerCreate(config: NativeTrainingConfig): boolean
/** Get current training state */
export declare function trainerGetState(): NativeTrainingState | null
/** Add a transition to the replay buffer */
export declare function trainerAddTransition(state: Array<number>, action: number, reward: number, nextState: Array<number>, done: boolean, decisionType: number): boolean
/** Get current replay buffer size */
export declare function trainerBufferSize(): number
/** Perform N training steps and return average loss */
export declare function trainerTrainSteps(numSteps: number, gamesPlayed: number): TrainStepResult
/** Result of training steps */
export interface TrainStepResult {
  stepsCompleted: number
  avgLoss: number
}
/** Request training to stop */
export declare function trainerRequestStop(): boolean
/** Check if training should stop */
export declare function trainerShouldStop(): boolean
/** Get weights as flat vector */
export declare function trainerGetWeights(): Array<number>
/** Set weights from flat vector */
export declare function trainerSetWeights(weights: Array<number>): boolean
/** Initialize DRL strategy for a player */
export declare function drlStrategyInit(playerIndex: number, epsilon: number, seed?: number | undefined | null): boolean
/** Set epsilon for DRL strategy */
export declare function drlStrategySetEpsilon(epsilon: number): boolean
/** Get action from DRL strategy given features */
export declare function drlStrategyGetAction(features: Array<number>, decisionType: string): number
/** Model metadata exposed to JS */
export interface NativeModelMetadata {
  /** Model version */
  version: string
  /** Input dimension */
  inputDim: number
  /** Hidden layer dimension */
  hiddenDim: number
  /** Value stream hidden dimension */
  valueHidden: number
  /** Advantage stream hidden dimension */
  advantageHidden: number
  /** Number of training steps */
  trainingSteps: number
  /** Number of games played */
  gamesPlayed: number
  /** Final epsilon value */
  finalEpsilon: number
  /** Average loss at save time */
  avgLoss: number
  /** Win rate at save time */
  winRate: number
  /** Timestamp of save */
  timestamp: string
}
/** Result of loading model weights with metadata (NAPI-compatible) */
export interface NativeModelLoadResult {
  /** Model weights as f64 array */
  weights: Array<number>
  /** Optional metadata if available */
  metadata?: NativeModelMetadata
}
/** Save model weights to file */
export declare function modelSave(path: string, weights: Array<number>): boolean
/** Save model checkpoint with metadata */
export declare function modelSaveCheckpoint(path: string, weights: Array<number>, trainingSteps: number, gamesPlayed: number, epsilon: number, avgLoss: number, winRate: number): boolean
/** Load model weights from file */
export declare function modelLoad(path: string): Array<number> | null
/** Load model weights and metadata from file */
export declare function modelLoadWithMetadata(path: string): NativeModelLoadResult | null
/** Check if model file exists */
export declare function modelExists(path: string): boolean
/** Save trainer's current model to file */
export declare function trainerSaveModel(path: string): boolean
/** Get model metadata without loading weights */
export declare function modelGetMetadata(path: string): NativeModelMetadata | null
/** Trace configuration for diagnostic output */
export interface NativeTraceConfig {
  /** Game/transition collection: decisions, actions, rewards */
  game: boolean
  /** Replay buffer: sampling stats, priority distribution */
  buffer: boolean
  /** Training step: Q-values, TD errors, loss, gradients */
  training: boolean
  /** Weight synchronization: DuelingDQN <-> FastDQN */
  weights: boolean
  /** Feature extraction: validation, NaN/Inf checks */
  features: boolean
}
/**
 * Set trace configuration for diagnostic output
 * Use --trace=game,training,buffer or --debug for all
 */
export declare function setTraceConfig(config: NativeTraceConfig): void
/** Thibot parameters for genetic optimization */
export interface NativeThibotParams {
  jokerKeepScore: number
  existingPairBonus: number
  goodPairChanceBonus: number
  lowPairChanceBonus: number
  deadRankPenalty: number
  sequencePartBonus: number
  potentialSequenceBonus: number
  jokerSequenceBonus: number
  closeWithJokerBonus: number
  valueScoreWeight: number
  cardsScoreWeight: number
  potentialDivisor: number
  jokerPlayPenalty: number
  zapzapPotentialBonus: number
  discardJokerScore: number
  lowPointsBase: number
  pairCompletionBonus: number
  threeOfKindBonus: number
  sequenceCompletionBonus: number
  deadRankDiscardPenalty: number
  discardThreshold: number
  defensiveThreshold: number
  zapzapSafeHandSize: number
  zapzapModerateHandSize: number
  zapzapModerateValueThreshold: number
  zapzapRiskyHandSize: number
  zapzapRiskyValueThreshold: number
  zapzapSafeValueThreshold: number
  futureValueDiscount: number
  riskPenaltyMultiplier: number
  coordinationThreshold: number
  holdPairForThreeBonus: number
  holdSequenceForExtendBonus: number
}
/** Set Thibot parameters for genetic optimization */
export declare function thibotSetParams(params: NativeThibotParams): void
/** Get default Thibot parameters */
export declare function thibotGetDefaultParams(): NativeThibotParams
